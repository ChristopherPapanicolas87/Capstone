---
title: "Natural Processing Langugage and Swift Key-THe importance of Word Predicition"
author: "Christopher Papanicolas"
date: "May 16, 2017"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
```

### Introduction

##### The Johns Hopkins Data Specialization Course and Swift Key partnered up to help students apply their skills in data science. A large set of structured text( text corpus) from different written documents will be explored and shiny app will be produced that will allow one to predict the next word in a structured sentence or phrase. This document is a milestone report where we will clean and explore the data, and discuss a plan to go about building a prediction model. 

#### 1. Libraries for project: The tm package plays an important role in cleaning up the text file of filler words, punctuation, whitespace, fillerwords, and capital letters. The tokenization package helps produce the n-grams of words to carry out exploratory analysis. The Rweka pakage is also used  to produce the word grouping (n-grams). The stringi is also important for assessing text and character length. The SnowBallC package is used to assess wording based on word root.l The tokenizer package was important to sperate words by n-grams into sentences, word roots, and word groups.We will also use wordcloud to produce visuals of different word grams. 


```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(tm) #text mining 
library(NLP)
library(stringi) #string processing for various lines
library(RColorBrewer) #Color Paallete
library(ggplot2)
library(lattice)
library(rJava)
library(RWeka)
library(RWekajars) ##Tokenization of data into the uni, bi, and multi-grams
library(SnowballC) ##Collapsing words based on word root 
library(qdap) ##Word, sentence, and structural frequency for text prediction 
library(tokenizers)
library(ANLP)
```


#### 2. Read each file: Each text file in the directory, including the USNews, USBlogs, and twitter text was read using the ReadLInes function. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
USNews<- file("en_US.news.txt", open="rb") # open for reading in binary mode
news <- readLines(USNews, encoding = "UTF-8", skipNul=TRUE)
close(USNews)
USblogs <- file("en_US.blogs.txt", open = "rb")
blogs <- readLines (USblogs, encoding = "UTF-8", skipNul=TRUE)
close(USblogs)
UStwitter <- file("en_US.twitter.txt", open = "rb")
twitter <- readLines(UStwitter, encoding = "UTF-8", skipNul = TRUE)
close(UStwitter)
```

#### 3. General SUmmary of each file: A summary of each of the text files including memory spaace and length of the document. In addition, the number of characters in the longest line of each text document. 
```{r, echo=FALSE, message=FALSE, warning=FALSE}
##size in megabytes
f1 <- file.size("en_US.news.txt")/(1024^2)
f2 <- file.size ("en_US.blogs.txt")/(1024^2)
f3 <- file.size("en_US.twitter.txt")/(1024^2)

Filesize <- c(f1,f2,f3)
Source<- c("News", "blogs", "twitter")

txtsize <- data.frame(Source, Filesize)
##Number of lines in each document 
a <- length(news) 
b <- length(blogs)
c <- length(twitter)

print(txtsize)

Length <- c(a,b,c)

numberoflines <- data.frame(Source, Length)

print(numberoflines)
##Number of words in each document 
library(stringi)
s1 <- sum(stri_count_words(news))
s2 <- sum(stri_count_words(blogs))
s3 <- sum(stri_count_words(twitter))

sumofwords <- c(s1,s2,s3)

Wordcount <- data.frame(Source, sumofwords)
print(Wordcount)
##Calculate the line with max characters in it 

nchar1<- max(nchar(news))
nchar2 <- max(nchar(blogs))
nchar3 <- max(nchar(twitter))
values <- c(nchar1,nchar2, nchar3)
Lengthoflongestline<-data.frame(Source, values)
print(Lengthoflongestline)
```

#### 4. The data is gigantic so we need to subset a sample of each txt file. The words that come out of each text file is random. We created a random sample of all three text files and merged them into a single file called samplefinal. We selected 2000 lines of each of the three documents and then cummulated them together into one document. The length of the document was then calculated to be 6000. A text file was produced for each of the samples collected from the original documents. In the final project we will build a more comprehensive corpus. We will build it in chunks in order to save memory.  

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(1234)
sampTwitter <- sample(twitter, size=2000, replace=FALSE)
sampnews <- sample(news, size=2000, replace =FALSE)
sampblogs <- sample(blogs, size= 2000, replace=FALSE)
sampfinal <- c(sampTwitter, sampnews, sampblogs)
subsettotal <- length(sampfinal)
writeLines(sampTwitter, "./twitterref.txt")
writeLines(sampnews, "./newsref.txt")
writeLines(sampblogs, "./blogref.txt")
writeLines(sampfinal, "./sampfinal.txt")
```

#### 5. Cleaning and Refining Data: The final documents with all the text is stripped of white space, converted to UTF-8 text formation. IN addition, the document text is converted to lower case, numbers are removed, punctuation is removed, profanity is filtered, URL links are removed, and filler and stopwords are removed. Sampling of the text is outputed to inspect the data and make sure its clean. 
```{r, echo=FALSE, message=FALSE, warning=FALSE}


sampfinallist <- readLines("sampfinal.txt")
finalCorpus <- Corpus(VectorSource(sampfinallist))
finalCorpus <- tm_map(finalCorpus, stripWhitespace)
```

### More cleaning was conducted as described above
#### 1. Converting text to UTF-8 
#### 2. Removing punctuation 
#### 3. removing profanity 
#### 4. Removing hyperlinks 
#### 5. Removing stopwords 

```{r, echo=FALSE, message=FALSE, warning=FALSE}

## LowerCase and Punction removal

## Sampfinal -->words from all three documents 

finalCorpus <- tm_map(finalCorpus, content_transformer(function(x) iconv(x, to = "UTF-8", sub = "byte")))
finalCorpus <- tm_map(finalCorpus, content_transformer(tolower))
finalCorpus <- tm_map(finalCorpus, content_transformer(removePunctuation), preserve_intra_word_dashes = TRUE)

## Sample final 
badwords <- readLines("offensive.txt")
finalCorpus <- tm_map(finalCorpus, removeWords, badwords)
## Remove Numbers 

# FInal Sample 

finalCorpus <- tm_map(finalCorpus, content_transformer(removeNumbers))
## Remove URL

z <- function(x) gsub("http:*", "", x)


finalCorpus <- tm_map(finalCorpus, content_transformer(z))

# Remove fillers 
library(tokenizers)
finalCorpus <- tm_map(finalCorpus, removeWords, stopwords(language=c("en")))
finalCorpus2 <- sample(finalCorpus, size = 200, replace = TRUE)

##Vizualize the document 

m4 <- for (i in 1:8) {print (finalCorpus[[i]]$content)}

writeLines(as.character(finalCorpus), con="mycorpus.txt")
## Save output into a text file 

saveRDS(finalCorpus,file = "./finalCorpus.RData") 

```




#### 6. Tokenization and Exploratory Data: THe final clean text is sunk into a new text file. The file is then read into R and tokenized. THis allowed us to produce the n-grams (including Onegrams, Bigrams, and Trigrams). We also conducted a onegrams using the word stems of the words. We then took the top 50 most frequent words or word roots for each n-gram and graphed them with bar graphs. We also produced word clouds for each n-gram as well. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tokenizers)
cleancorps <- readLines("mycorpus.txt", encoding ="UTF-8", skipNul = TRUE)
cleancorpstoken1 <- tokenize_words(cleancorps)
cleancorpstoken2 <- tokenize_word_stems(cleancorps,stopwords = c("en"))
cleancorpstoken3 <- tokenize_ngrams(cleancorps, n=1L, n_min =1, ngram_delim =" \\r\\n\\t.,;:\"()?!")
cleancorpstoken4 <- tokenize_ngrams(cleancorps, n=2L, n_min =2, ngram_delim =" \\r\\n\\t.,;:\"()?!")

Onegram <- NGramTokenizer(cleancorps, Weka_control(min = 1, max = 1,delimiters = " \\r\\n\\t.,;:\"()?!"))
Bigram <- NGramTokenizer(cleancorps, Weka_control(min = 2, max = 2,delimiters = " \\r\\n\\t.,;:\"()?!"))
Trigram <- NGramTokenizer(cleancorps, Weka_control(min = 3, max = 3,delimiters = " \\r\\n\\t.,;:\"()?!"))
Stem_gram <- NGramTokenizer(cleancorpstoken2,Weka_control(min = 1, max = 1, delimiters = " \\r\\n\\t.,;:\"()?!"))

Tab_onegram <- data.frame(table(Onegram))
Tab_bigram <- data.frame(table(Bigram))
Tab_trigram <- data.frame(table(Trigram))
Tab_stem <- data.frame(table(Stem_gram))

OnegramGrp <- Tab_onegram[order(Tab_onegram$Freq,decreasing = TRUE),]
BigramGrp <- Tab_bigram[order(Tab_bigram$Freq,decreasing = TRUE),]
TrigramGrp <- Tab_trigram[order(Tab_trigram$Freq,decreasing = TRUE),]
StemgramGrp <- Tab_stem[order(Tab_stem$Freq,decreasing = TRUE),]

n1 <- nrow(Tab_onegram)
n2 <- nrow(Tab_bigram)
n3 <- nrow(Tab_trigram)
n4 <- nrow(Tab_stem)
Count <- c(n4, n1,n2,n3)

NGram <- c("wordstem", "1gram", "2Gram","3gram")

Freq <- c(n4/n4, n1/n4, n2/n4, n3/n4)

TableGram <- data.table::data.table(NGram, Count, Freq)

print(as.data.frame(TableGram))
pl <- ggplot(TableGram, aes(reorder(x = NGram, -Count), y = Count))+geom_bar(stat = "identity")
print(pl)             
OneSamp <- OnegramGrp[1:50,]
colnames(OneSamp) <- c("Word","Frequency")
BiSamp <- BigramGrp[1:50,]
colnames(BiSamp) <- c("Word","Frequency")
TriSamp <- TrigramGrp[1:50,]
colnames(TriSamp) <- c("Word","Frequency")
StemSamp <- StemgramGrp [1:50,]
colnames(StemSamp) <- c("Word","Frequency")

print(OneSamp,BiSamp,TriSamp,StemSamp)

graph <- ggplot(OneSamp, aes(x = reorder(Word, -Frequency), y = Frequency)) + 
  geom_bar(stat = "identity", fill="green")+xlab("Word")+coord_flip() + ggtitle("Frequency of Onegram")
graph1 <- ggplot(BiSamp, aes(x = reorder(Word, -Frequency), y = Frequency)) + 
  geom_bar(stat = "identity", fill="blue")+xlab("Word")+coord_flip() + ggtitle("Freqeuncy of Bigrm ")
graph3 <- ggplot(TriSamp, aes(x = reorder(Word, -Frequency), y = Frequency)) + 
  geom_bar(stat = "identity", fill="blue")+xlab("Word")+coord_flip()+ggtitle("Frequency of Trigram")
graph4 <- ggplot(StemSamp, aes(x = reorder(Word, -Frequency), y = Frequency)) + 
  geom_bar(stat = "identity", fill="black")+xlab("Word")+coord_flip()+ggtitle("Frequency based on Word Stem ")
print(graph)
print(graph1)
print(graph3)
print(graph4)
Wordcloud1 <- par(mar = c(0, 6, 8, 6) + 0.5)
wordcloud::wordcloud(OnegramGrp$Onegram, OnegramGrp$Freq, max.words = 200, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
wordcloud2 <-par(mar = c(0, 6, 8, 6) + 0.5)
wordcloud2 <- wordcloud::wordcloud(BigramGrp$Bigram, BigramGrp$Freq, max.words = 200, random.order = FALSE, colors = brewer.pal(8, 
    "Dark2"))
wordcloud3 <-par(mar = c(0, 6, 8, 6) + 0.5)
wordcloud3 <- wordcloud::wordcloud(TrigramGrp$Trigram, TrigramGrp$Freq, max.words = 200, random.order = FALSE, colors = brewer.pal(8, 
    "Dark2"))

wordcloud4 <-par(mar = c(0, 6, 8, 6) + 0.5)
wordcloud4 <- wordcloud::wordcloud(StemgramGrp$Stem_gram, StemgramGrp$Freq, max.words = 200, random.order = FALSE, colors = brewer.pal(8, 
    "Dark2"))

print(Wordcloud1)
print(wordcloud2)
print(wordcloud3)
print(wordcloud4)
```
#### Here we assessed the coverage of the dictionary in relation to the number of unique n-gamps. The number onegrams covered 
```{r, echo=FALSE, message=FALSE, warning=FALSE}
 OneGramup<- within(OnegramGrp,tot <- cumsum(OnegramGrp$Freq))
as.data.frame(OneGramup)
OneGramup$Coverage <- (OneGramup$tot/(sum(OneGramup$Freq))*100)
OneGramup$value <- 1:nrow(OneGramup)
Oneplot <- xyplot(OneGramup$Coverage~OneGramup$value,xlab="Number of unique Grams",ylab="Coverage")
Bigramup <- within (BigramGrp, tot2 <- cumsum(BigramGrp$Freq))
as.data.frame(Bigramup)
Bigramup$coverage <- (Bigramup$tot2/(sum(Bigramup$Freq))*100)
Bigramup$value <- 1:nrow(Bigramup)
Biplot <- xyplot(Bigramup$coverage~Bigramup$value,xlab="Number of unique BiGrams",ylab="Coverage")
Trigramup <- within(TrigramGrp, tot3 <- cumsum(TrigramGrp$Freq))
as.data.frame(Trigramup)
Trigramup$Coverage <- (Trigramup$tot3/(sum(Trigramup$Freq))*100)
Trigramup$value <- 1: nrow(Trigramup)
Triplot <-xyplot(Trigramup$Coverage ~ Trigramup$value, xlab="Numberof Trigrams", ylab="Coverage")
Stemgramup <- within(StemgramGrp, tot4 <- cumsum(StemgramGrp$Freq))
as.data.frame(Stemgramup)
Stemgramup$Coverage  <- (Stemgramup$tot4/(sum(Stemgramup$Freq))*100)
Stemgramup$value <- 1:nrow(Stemgramup)
Stemplot <- xyplot(Stemgramup$Coverage ~Stemgramup$value, xlab= "Stemgrams", ylab="Coverage")
print(Stemplot)
print(Oneplot)
print(Biplot)
print(Triplot)

````

#### The bigger the n gram, the more linear the behavior is between the number of n grams and the coverage in the dictionary. The one grams and stem grams are more steap and then level off. 

### The next step will be to: 
#### 1. Continue to build the n-gram tables in chunks to save memory space. 
#### 2.Build a prediction model using n-gram tables 
#### 3.; Build a prediction app 
#### 4. Refine exploratory data 
